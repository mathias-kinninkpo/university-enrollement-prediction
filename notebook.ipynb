{"cells":[{"source":"# Data Scientist Associate Practical Exam Submission\n\nUse this template to complete your analysis and write up your summary for submission.\n","metadata":{},"id":"1e20b471-43f1-4e12-afb1-aa7825d8cc69","cell_type":"markdown"},{"source":"## Task 1 \n\n**1. course_id**:\ncourse_id\nThis column does not exactly match the description in the table. In the table, it was mentioned that course_id should be of nominal type. However, we observed that it is of type int64. It is important to note that the values are unique in the dataset!\n\n**This column does not contain any missing values**.\n\n\nTo correct the type discrepancy, it needs to be transformed into categorical type to avoid confusing it as a continuous value during model training (using the astype() method of a Pandas DataFrame).\n\n\n\n\n**2. course_type**:\nFor this column, the values do not correspond to the description in the table. Ideally, it should be of nominal type according to the table, but we noticed that it is of type \"object\".\n\nThis column **does not contain any missing values**: 1850/1850 elements\nTo fulfill the criteria from the table, it should be transformed into \"category\" type.\n\nThis is done using the following code: \n```\ndata['course_type'] = data['course_type'].astype(\"category\")\n``` \n\n\n\n\n\n**3. year**:\nValues match the given description (discrete values from 2011 to 2022).\n**0 missing value**\n\n\n\n\n**4. enrollment_count**:\nValues match the given description (discrete representing the number of enrolled students).\n**0 missing value**\n\n\n\n\n**5. pre_score**:\nThe pre_score column is of type \"object\". Consequently, it does not fulfill the criteria in the table.\n\nThere are **130 missing values** that are represented as \"-\". These values have been replaced with \"0\".\n\nTo comply with the criteria from the table, these values need to be converted to float type to have continuous values, as indicated in the table, using the astype() method.\n\n\n\n\n\n**6. post_score**:\nValues match the given description (continuous representing the average score on the post-course exam). its type is \"float\"\n\nThe column contains **185 missing values**.\nThese values have been replaced with 0, as indicated in the table\n\n\n\n\n**7. pre_requirement**:\nThe values in this column do not correspond to the description in the table; they are of type \"object\".\n\nThis column contains **89 missing values**.\nThe missing values have been replaced with \"None,\" as indicated using \n```\ndata['pre_requirement'] = data['pre_requirement'].replace(\"NaN\", \"None\")\n```\n\nwe transformed this column into \"category\" type using the following code:\n\n```\ndata['pre_requirement'] = data['pre_requirement'].astype(\"category\")\n```\n\n\n**8. department**:\nThe values in this column do not correspond to the description in the table; they are of type \"object\".\n\nThis column contains **0 missing values**\n.\nTo address this issue, we first replaced \"Math\" with \"Mathematics,\" and then we transformed this column into \"category\" type using the following code:\n```\ndata['department'] = data['department'].replace(\"Math\", \"Mathematics\")\n```\n```\ndata['department'] = data['department'].astype(\"category\")\n```","metadata":{},"id":"836353de-534f-460d-b9ce-e92cc3646c65","cell_type":"markdown"},{"source":"## Task 2\n\nCount: The total number of data points (enrollment counts) in the dataset is 1850.\n\nMean (Average): The mean enrollment count is approximately 231.57 students. This value represents the central tendency of the distribution, indicating that, on average, the number of students enrolled in courses is around 232.\n\nStandard Deviation: The standard deviation is approximately 36.99. This value measures the spread or variability of enrollment counts around the mean. A higher standard deviation suggests that the enrollment counts are more spread out from the mean value of 232, while a lower standard deviation indicates that the counts are more concentrated around the mean.\n\nMinimum: The minimum enrollment count in the dataset is 154 students. This is the smallest number of students enrolled in a course.\n\n25th Percentile (Q1): The 25th percentile, also known as the first quartile, is 185. This means that 25% of the enrollment counts are below or equal to 185. It provides a measure of the lower range of enrollment counts.\n\nMedian (50th Percentile, Q2): The median enrollment count is 251. This is the middle value in the dataset, where 50% of the enrollment counts are below or equal to 251. The median is less sensitive to extreme values and provides a measure of the central tendency.\n\n75th Percentile (Q3): The 75th percentile, also known as the third quartile, is 261. This means that 75% of the enrollment counts are below or equal to 261. It provides a measure of the upper range of enrollment counts.\n\nMaximum: The maximum enrollment count in the dataset is 267 students. This is the largest number of students enrolled in a course.\n \nOverall, the distribution of \"enrollment_count\" reveals that the majority of courses have enrollments centered around the mean value, with a relatively small standard deviation. The university offers a variety of courses, ranging from smaller classes to larger ones.\n \n \n \n \n \n The following diagrams provide more information\n \n \n histogram\n \n ![visualization_enrollment_count](visualization_enrollment_count.png)\n \n \n \n boxplot\n \n \n![boxplot_enrollment_count](boxplot_enrollment_count.png)\n ","metadata":{},"id":"0944d9ab-ff5a-4dd3-bd52-e27b91d68581","cell_type":"markdown"},{"source":"## Task 3\nNumber of Online Courses: There are 1375 courses labeled as \"online.\"\nNumber of Classroom Courses: There are 475 courses labeled as \"classroom.\"\n\n![couse_type_visualization](couse_type_visualization.png)\n\na. Course Type with the Most Observations: The course type with the most observations is \"online,\" as it has 1375 courses associated with it.\n\nb. Balance Across Types: **The observations are not balanced across the types.** \"Online\" courses significantly outnumber \"classroom\" courses, with a difference of 1375 - 475 = 900 courses. This indicates an imbalance, suggesting that the university offers a significantly higher number of online courses compared to classroom-based courses.","metadata":{},"id":"c509bc83-41a5-42cf-a865-08dcf8229a16","cell_type":"markdown"},{"source":"## Task 4\nBased on the aggregate functions for enrollment count for each type of course:\n\n- **Classroom Courses:**\n  - Minimum Enrollment Count: 154\n  - Maximum Enrollment Count: 190\n  - Mean Enrollment Count: 170.43 (rounded to two decimal places)\n  - Median Enrollment Count: 170.0\n\n- **Online Courses:**\n  - Minimum Enrollment Count: 231\n  - Maximum Enrollment Count: 267\n  - Mean Enrollment Count: 252.69 (rounded to two decimal places)\n  - Median Enrollment Count: 256.0\n\n**Comparison:**\n\nThe aggregate statistics reveal significant differences between classroom and online courses in terms of enrollment counts:\n\n1. **Enrollment Range:** Online courses have a wider range of enrollment counts, with both the minimum and maximum counts higher than those of classroom courses. This suggests that online courses tend to attract more students at both ends of the spectrum.\n\n2. **Central Tendency:** The mean and median enrollment counts for online courses are higher than those for classroom courses, indicating that online courses generally have higher enrollments on average.\n\n3. **Spread of Enrollment:** Classroom courses have a smaller spread of enrollments, as indicated by the narrower interquartile range (from Q1 to Q3) and a less significant difference between the median and mean values.\n\nOverall, the aggregate functions highlight the distinct enrollment characteristics between classroom and online courses. The online courses, on average, tend to have higher enrollments and a wider variation in student numbers compared to classroom courses. The differences in enrollment patterns between the two course types could be attributed to factors such as accessibility, flexibility, and student preferences for online learning.\n\n\n\nThe following diagrams better illustrate this comparison\n\n\n\n![comparaison1](comparaison1.png) \n\n\n![comparaison2](comparaison2.png)\n\n","metadata":{},"id":"17cca003-71ad-4d7e-989b-09aa51ae0aba","cell_type":"markdown"},{"source":"## Task 5\n\n\nThe university's task of predicting the number of students who will enroll in a course is a **regression** machine learning problem. Regression is the ideal choice because it focuses on making continuous numeric predictions, precisely what the university needs to forecast the exact enrollment count for their courses. Classification, which predicts categorical labels, and clustering, which groups data points without predicting a target variable, are not suitable for this scenario.","metadata":{},"id":"66e07a44-f276-4516-87ce-e460d3f1a33a","cell_type":"markdown"},{"source":"## Task 6\n\nThe choice of the Linear Regression model with one-hot encoding is due to its simplicity, interpretability, and flexibility. It serves as a good baseline for predicting enrollment counts, and its performance can be compared to more complex models. Additionally, one-hot encoding allows us to handle categorical features effectively.","metadata":{},"id":"8b20fa2e-da87-4cf4-9591-157a0837891e","cell_type":"markdown"},{"source":"#Import of all modules and packages\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\n\n\n# data manipulation\nuniversty_data = pd.read_csv(\"university_enrollment_2306.csv\")\nuniversty_data['course_type'] = universty_data['course_type'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].replace(\"NaN\", \"None\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].fillna(\"None\")\nuniversty_data[\"pre_score\"] =  universty_data[\"pre_score\"].replace(\"-\", \"0\")\nuniversty_data['pre_score'] = universty_data[\"pre_score\"].astype(float)\nuniversty_data['department'] = universty_data['department'].replace(\"Math\", \"Mathematics\")\nuniversty_data['department'] = universty_data['department'].astype(\"category\")\nuniversty_data['post_score'].fillna(0, inplace=True)\n\n# Extract the features (input) and target variable (output)\nX = universty_data[['year', 'pre_score', 'course_type', 'post_score', 'pre_requirement', 'department']]\ny = universty_data['enrollment_count']\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the preprocessing steps for categorical variables\ncategorical_features = ['course_type', 'pre_requirement', 'department']\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps for both numeric and categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Create the Linear Regression model with preprocessing pipeline\nmodel = Pipeline(steps=[('preprocessor', preprocessor),\n                        ('regressor', LinearRegression())])\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model's performance using Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\n","metadata":{"executionCancelledAt":null,"executionTime":2951,"lastExecutedAt":1690653869408,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Import of all modules and packages\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\n\n\n# data manipulation\nuniversty_data = pd.read_csv(\"university_enrollment_2306.csv\")\nuniversty_data['course_type'] = universty_data['course_type'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].replace(\"NaN\", \"None\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].fillna(\"None\")\nuniversty_data[\"pre_score\"] =  universty_data[\"pre_score\"].replace(\"-\", \"0\")\nuniversty_data['pre_score'] = universty_data[\"pre_score\"].astype(float)\nuniversty_data['department'] = universty_data['department'].replace(\"Math\", \"Mathematics\")\nuniversty_data['department'] = universty_data['department'].astype(\"category\")\nuniversty_data['post_score'].fillna(0, inplace=True)\n\n# Extract the features (input) and target variable (output)\nX = universty_data[['year', 'pre_score', 'course_type', 'post_score', 'pre_requirement', 'department']]\ny = universty_data['enrollment_count']\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the preprocessing steps for categorical variables\ncategorical_features = ['course_type', 'pre_requirement', 'department']\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps for both numeric and categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Create the Linear Regression model with preprocessing pipeline\nmodel = Pipeline(steps=[('preprocessor', preprocessor),\n                        ('regressor', LinearRegression())])\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model's performance using Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\n","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"76b4446c-9c17-4aa7-8119-8d33c51a6593","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Mean Squared Error: 0.09890894503206821\n"}]},{"source":"## Task 7\nThe code performs hyperparameter tuning using GridSearchCV to find the best settings for the Stacking Regressor. It defines a preprocessing pipeline, base models (RandomForestRegressor and LinearRegression), and a Stacking Regressor that combines the base models. GridSearchCV searches for the optimal hyperparameters (e.g., n_estimators, max_depth, fit_intercept) using cross-validation to minimize the mean squared error. The best model is then evaluated on the test set, providing an optimized solution for predicting enrollment counts.","metadata":{},"id":"b7f0dcc4-98c7-43fe-8ee2-629aac60c421","cell_type":"markdown"},{"source":"#Import of all modules and packages\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# data manipulation\nuniversty_data = pd.read_csv(\"university_enrollment_2306.csv\")\nuniversty_data['course_type'] = universty_data['course_type'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].replace(\"NaN\", \"None\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].fillna(\"None\")\nuniversty_data[\"pre_score\"] =  universty_data[\"pre_score\"].replace(\"-\", \"0\")\nuniversty_data['pre_score'] = universty_data[\"pre_score\"].astype(float)\nuniversty_data['department'] = universty_data['department'].replace(\"Math\", \"Mathematics\")\nuniversty_data['department'] = universty_data['department'].astype(\"category\")\nuniversty_data['post_score'].fillna(0, inplace=True)\n\n# Extract the features (input) and target variable (output)\nX = universty_data[['year', 'pre_score', 'course_type', 'post_score', 'pre_requirement', 'department']]\ny = universty_data['enrollment_count']\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the preprocessing steps for numeric and categorical variables\nnumeric_features = ['year', 'pre_score', 'post_score']\ncategorical_features = ['course_type', 'pre_requirement', 'department']\n\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps for both numeric and categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Define the base models\nbase_models = [\n    ('rf', RandomForestRegressor(random_state=42)),\n    ('lr', LinearRegression())\n]\n\n# Create the Stacking Regressor model with preprocessing pipeline\nmodel = StackingRegressor(estimators=base_models, final_estimator=LinearRegression())\n\n# Create the full pipeline with preprocessor and the Stacking Regressor\nfull_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('stacking_regressor', model)])\n\n# Define hyperparameters to tune\nparam_grid = {\n    'stacking_regressor__rf__n_estimators': [50, 100, 150],\n    'stacking_regressor__rf__max_depth': [None, 10, 20],\n    'stacking_regressor__lr__fit_intercept': [True, False]\n}\n\n# Perform GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train, y_train)\n\n# Get the best model and its hyperparameters\nbest_model = grid_search.best_estimator_\nbest_params = grid_search.best_params_\n\n# Make predictions on the test set using the best model\ny_pred = best_model.predict(X_test)\n\n# Evaluate the model's performance using Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Mean Squared Error:\", mse)\n\n","metadata":{"executionCancelledAt":null,"executionTime":94825,"lastExecutedAt":1690656665867,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Import of all modules and packages\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# data manipulation\nuniversty_data = pd.read_csv(\"university_enrollment_2306.csv\")\nuniversty_data['course_type'] = universty_data['course_type'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].replace(\"NaN\", \"None\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].fillna(\"None\")\nuniversty_data[\"pre_score\"] =  universty_data[\"pre_score\"].replace(\"-\", \"0\")\nuniversty_data['pre_score'] = universty_data[\"pre_score\"].astype(float)\nuniversty_data['department'] = universty_data['department'].replace(\"Math\", \"Mathematics\")\nuniversty_data['department'] = universty_data['department'].astype(\"category\")\nuniversty_data['post_score'].fillna(0, inplace=True)\n\n# Extract the features (input) and target variable (output)\nX = universty_data[['year', 'pre_score', 'course_type', 'post_score', 'pre_requirement', 'department']]\ny = universty_data['enrollment_count']\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the preprocessing steps for numeric and categorical variables\nnumeric_features = ['year', 'pre_score', 'post_score']\ncategorical_features = ['course_type', 'pre_requirement', 'department']\n\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps for both numeric and categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Define the base models\nbase_models = [\n    ('rf', RandomForestRegressor(random_state=42)),\n    ('lr', LinearRegression())\n]\n\n# Create the Stacking Regressor model with preprocessing pipeline\nmodel = StackingRegressor(estimators=base_models, final_estimator=LinearRegression())\n\n# Create the full pipeline with preprocessor and the Stacking Regressor\nfull_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('stacking_regressor', model)])\n\n# Define hyperparameters to tune\nparam_grid = {\n    'stacking_regressor__rf__n_estimators': [50, 100, 150],\n    'stacking_regressor__rf__max_depth': [None, 10, 20],\n    'stacking_regressor__lr__fit_intercept': [True, False]\n}\n\n# Perform GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train, y_train)\n\n# Get the best model and its hyperparameters\nbest_model = grid_search.best_estimator_\nbest_params = grid_search.best_params_\n\n# Make predictions on the test set using the best model\ny_pred = best_model.predict(X_test)\n\n# Evaluate the model's performance using Mean Squared Error (MSE)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Mean Squared Error:\", mse)\n\n","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"id":"3d5953c8-9f51-426c-a597-b41bd742e4d1","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Best Hyperparameters: {'stacking_regressor__lr__fit_intercept': False, 'stacking_regressor__rf__max_depth': 10, 'stacking_regressor__rf__n_estimators': 100}\nMean Squared Error: 0.09918617592240507\n"}]},{"source":"## Task 8\nIn part 6, I chose the Linear Regression model with one-hot encoding for categorical variables as a baseline model. Linear Regression is a simple and interpretable regression technique widely used for predicting numeric values. One-hot encoding is applied to handle categorical variables, converting them into numerical representations that the model can work with. The choice of Linear Regression as the baseline model allows us to establish a straightforward prediction approach, which is easy to interpret and provides a good starting point for comparison.\n\nIn part 7, I opted for the Stacking Regressor with hyperparameter tuning using GridSearchCV as a comparison model. The Stacking Regressor is a powerful ensemble learning technique that combines the predictions of multiple base models (RandomForestRegressor and Linear Regression in this case) to improve predictive performance. Hyperparameter tuning with GridSearchCV allows us to systematically search for the best combination of hyperparameters for both the base models and the final estimator (Linear Regression) of the Stacking Regressor. By leveraging the strengths of different models and optimizing their parameters, the Stacking Regressor aims to provide more accurate predictions than a single model alone.\n\nIn summary, the choice of Linear Regression with one-hot encoding as the baseline model and the Stacking Regressor with hyperparameter tuning as the comparison model offers a balanced approach. The baseline model gives us a clear and interpretable prediction, suitable for initial insights, while the Stacking Regressor with hyperparameter tuning aims to leverage the best of both base models and further enhance the predictive capabilities, potentially providing more accurate enrollment count predictions.","metadata":{},"id":"5cec4b2d-86fd-4ce3-9f9c-e9ab2ec0e436","cell_type":"markdown"},{"source":"## Task 9\nIn the comparison of the models, we evaluated the performance of Linear Regression and Stacking Regressor models for predicting enrollment counts. We used Mean Squared Error (MSE) to measure the accuracy of the predictions.","metadata":{},"id":"84fc2a09-ea39-4a6f-85b2-5d17957e1ac5","cell_type":"markdown"},{"source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score, mean_absolute_percentage_error\n\n\n# data manipulation\nuniversty_data = pd.read_csv(\"university_enrollment_2306.csv\")\nuniversty_data['course_type'] = universty_data['course_type'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].replace(\"NaN\", \"None\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].fillna(\"None\")\nuniversty_data[\"pre_score\"] =  universty_data[\"pre_score\"].replace(\"-\", \"0\")\nuniversty_data['pre_score'] = universty_data[\"pre_score\"].astype(float)\nuniversty_data['department'] = universty_data['department'].replace(\"Math\", \"Mathematics\")\nuniversty_data['department'] = universty_data['department'].astype(\"category\")\nuniversty_data['post_score'].fillna(0, inplace=True)\n\n\n# Extract the features (input) and target variable (output)\nX = universty_data[['year', 'pre_score', 'course_type', 'post_score', 'pre_requirement', 'department']]\ny = universty_data['enrollment_count']\n    \n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Part 6: Linear Regression Model\ncategorical_features = ['course_type', 'pre_requirement', 'department']\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features)\n    ])\nmodel_lr = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('regressor', LinearRegression())])\n\nmodel_lr.fit(X_train, y_train)\ny_pred_lr = model_lr.predict(X_test)\nmse_lr = mean_squared_error(y_test, y_pred_lr)\n\n# Part 7: Stacking Regressor Model\nnumeric_features = ['year', 'pre_score', 'post_score']\ncategorical_features = ['course_type', 'pre_requirement', 'department']\n\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nbase_models = [\n    ('rf', RandomForestRegressor(random_state=42)),\n    ('lr', LinearRegression())\n]\n\nmodel_sr = StackingRegressor(estimators=base_models, final_estimator=LinearRegression())\n\nfull_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('stacking_regressor', model_sr)])\n\nparam_grid = {\n    'stacking_regressor__rf__n_estimators': [50, 100, 150],\n    'stacking_regressor__rf__max_depth': [None, 10, 20],\n    'stacking_regressor__lr__fit_intercept': [True, False]\n}\n\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train, y_train)\n\nbest_model_sr = grid_search.best_estimator_\ny_pred_sr = best_model_sr.predict(X_test)\nmse_sr = mean_squared_error(y_test, y_pred_sr)\n\n# Print the results\nprint(\"Mean Squared Error (Linear Regression Model):\", mse_lr)\nprint(\"Mean Squared Error (Stacking Regressor Model):\", mse_sr)\n","metadata":{"executionCancelledAt":null,"executionTime":95043,"lastExecutedAt":1690661701743,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score, mean_absolute_percentage_error\n\n\n# data manipulation\nuniversty_data = pd.read_csv(\"university_enrollment_2306.csv\")\nuniversty_data['course_type'] = universty_data['course_type'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].replace(\"NaN\", \"None\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].astype(\"category\")\nuniversty_data['pre_requirement'] = universty_data['pre_requirement'].fillna(\"None\")\nuniversty_data[\"pre_score\"] =  universty_data[\"pre_score\"].replace(\"-\", \"0\")\nuniversty_data['pre_score'] = universty_data[\"pre_score\"].astype(float)\nuniversty_data['department'] = universty_data['department'].replace(\"Math\", \"Mathematics\")\nuniversty_data['department'] = universty_data['department'].astype(\"category\")\nuniversty_data['post_score'].fillna(0, inplace=True)\n\n\n# Extract the features (input) and target variable (output)\nX = universty_data[['year', 'pre_score', 'course_type', 'post_score', 'pre_requirement', 'department']]\ny = universty_data['enrollment_count']\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Part 6: Linear Regression Model\ncategorical_features = ['course_type', 'pre_requirement', 'department']\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features)\n    ])\nmodel_lr = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('regressor', LinearRegression())])\n\nmodel_lr.fit(X_train, y_train)\ny_pred_lr = model_lr.predict(X_test)\nmse_lr = mean_squared_error(y_test, y_pred_lr)\n\n# Part 7: Stacking Regressor Model\nnumeric_features = ['year', 'pre_score', 'post_score']\ncategorical_features = ['course_type', 'pre_requirement', 'department']\n\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nbase_models = [\n    ('rf', RandomForestRegressor(random_state=42)),\n    ('lr', LinearRegression())\n]\n\nmodel_sr = StackingRegressor(estimators=base_models, final_estimator=LinearRegression())\n\nfull_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('stacking_regressor', model_sr)])\n\nparam_grid = {\n    'stacking_regressor__rf__n_estimators': [50, 100, 150],\n    'stacking_regressor__rf__max_depth': [None, 10, 20],\n    'stacking_regressor__lr__fit_intercept': [True, False]\n}\n\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train, y_train)\n\nbest_model_sr = grid_search.best_estimator_\ny_pred_sr = best_model_sr.predict(X_test)\nmse_sr = mean_squared_error(y_test, y_pred_sr)\n\n# Print the results\nprint(\"Mean Squared Error (Linear Regression Model):\", mse_lr)\nprint(\"Mean Squared Error (Stacking Regressor Model):\", mse_sr)\n","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"246f2368-09ae-4cac-bdb5-7daacb0b2faf","cell_type":"code","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"Mean Squared Error (Linear Regression Model): 0.09890894503206821\nMean Squared Error (Stacking Regressor Model): 0.09918617592240507\n"}]},{"source":"## Task 10\nThe result, where the **Linear Regression model outperforms the Stacking Regressor model in terms of Mean Squared Error (MSE)**, could be attributed to several factors:\n\n1. Simplicity of Linear Regression: Linear Regression is a simple and interpretable model that assumes a linear relationship between the features and the target variable. In some cases, when the data exhibits a relatively linear relationship, Linear Regression can provide accurate predictions, especially if there are not many complex interactions between the features.\n\n2. Limited Complexity: Stacking Regressor, on the other hand, combines multiple base models, including RandomForestRegressor and Linear Regression. It has more complexity than the simple Linear Regression model. However, in this particular case, the additional complexity may not have significantly improved the predictive performance. The dataset may not have exhibited strong non-linear patterns or interactions that would have been better captured by the Stacking Regressor.\n\n3. Overfitting: The Stacking Regressor with hyperparameter tuning through GridSearchCV may have become more prone to overfitting the training data, especially with a limited amount of data available. Overfitting occurs when the model becomes too specialized to the training data and performs poorly on unseen data.\n\n4. Data Characteristics: The dataset may have been better suited for a simpler model like Linear Regression. The data's linearity and the relationship between features and target variable could align well with the assumptions of Linear Regression, making it a more appropriate choice for this specific prediction task.\n\n\n![compar](compar.png)\n","metadata":{},"id":"feb9e1eb-8913-43b8-a280-a369cf10631e","cell_type":"markdown"},{"source":"## ✅  When you have finished...\n- Publish your Workspace using the option on the left\n- Check the published version of your report:\n\t- Can you see everything you want us to grade?\n    - Are all the graphics visible?\n- Review the grading rubric. Have you included everything that will be graded?\n- Head back to the [Certification dashboard](https://app.datacamp.com/certification) to submit your practical exam","metadata":{},"id":"c42a2f84-876d-4002-9d9b-990873351e5f","cell_type":"markdown"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}